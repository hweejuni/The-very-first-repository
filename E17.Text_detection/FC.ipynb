{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테서랙트 써보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 테서랙트 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ sudo apt install tesseract-ocr   \n",
    "\n",
    "$ sudo apt install libtesseract-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 테서랙트 파이썬 wrapper 설치하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytesseract는 OS에 설치된 테서랙트를 파이썬에서 쉽게 사용할 수있도록 해주는 래퍼 라이브러리(wrapper library)입니다. 파이썬 내에서 컴퓨터에 설치된 테서랙트 엔진의 기능을 바로 쓸 수 있도록 해줍니다.   \n",
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 테서랙트로 문자 검출하고 이미지 자르기 (detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OCR Engine modes(–oem):\n",
    "# 0 - Legacy engine only.\n",
    "# 1 - Neural nets LSTM engine only.\n",
    "# 2 - Legacy + LSTM engines.\n",
    "# 3 - Default, based on what is available.\n",
    "\n",
    "# Page segmentation modes(–psm):\n",
    "# 0 - Orientation and script detection (OSD) only.\n",
    "# 1 - Automatic page segmentation with OSD.\n",
    "# 2 - Automatic page segmentation, but no OSD, or OCR.\n",
    "# 3 - Fully automatic page segmentation, but no OSD. (Default)\n",
    "# 4 - Assume a single column of text of variable sizes.\n",
    "# 5 - Assume a single uniform block of vertically aligned text.\n",
    "# 6 - Assume a single uniform block of text.\n",
    "# 7 - Treat the image as a single text line.\n",
    "# 8 - Treat the image as a single word.\n",
    "# 9 - Treat the image as a single word in a circle.\n",
    "# 10 - Treat the image as a single character.\n",
    "# 11 - Sparse text. Find as much text as possible in no particular order.\n",
    "# 12 - Sparse text with OSD.\n",
    "# 13 - Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "def crop_word_regions(image_path='./images/sample.png', output_path='./output'):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    recognized_data = pytesseract.image_to_data(\n",
    "        image, lang='eng',    # 한국어라면 lang='kor'\n",
    "        config=custom_oem_psm_config,\n",
    "        output_type=Output.DICT\n",
    "    )\n",
    "    \n",
    "    top_level = max(recognized_data['level'])\n",
    "    index = 0\n",
    "    cropped_image_path_list = []\n",
    "    for i in range(len(recognized_data['level'])):\n",
    "        level = recognized_data['level'][i]\n",
    "    \n",
    "        if level == top_level:\n",
    "            left = recognized_data['left'][i]\n",
    "            top = recognized_data['top'][i]\n",
    "            width = recognized_data['width'][i]\n",
    "            height = recognized_data['height'][i]\n",
    "            \n",
    "            output_img_path = os.path.join(output_path, f\"{str(index).zfill(4)}.png\")\n",
    "            print(output_img_path)\n",
    "            cropped_image = image.crop((\n",
    "                left,\n",
    "                top,\n",
    "                left+width,\n",
    "                top+height\n",
    "            ))\n",
    "            cropped_image.save(output_img_path)\n",
    "            cropped_image_path_list.append(output_img_path)\n",
    "            index += 1\n",
    "    return cropped_image_path_list\n",
    "\n",
    "\n",
    "work_dir = os.getenv('HOME')+'/aiffel/ocr_python'\n",
    "img_file_path = work_dir + '/test.jpg'   #테스트용 이미지 경로입니다. 본인이 선택한 파일명으로 바꿔주세요. \n",
    "\n",
    "cropped_image_path_list = crop_word_regions(img_file_path, work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 구현한 crop_word_regions() 함수는 여러분이 선택한 테스트 이미지를 받아서, 문자 검출을 진행한 후, 검출된 문자 영역을 crop한 이미지로 만들어 그 파일들의 list를 리턴하는 함수입니다.\n",
    "\n",
    "기본적으로 pytesseract.image_to_data() 를 사용합니다. 파이썬에서 편하게 사용하기 위해서 pytesseract 의 Output 을 사용해서 결과값의 형식을 딕셔너리(DICT) 형식으로 설정해주게 됩니다. 이렇게 인식된 결과는 바운딩 박스의 left, top, width, height 정보를 가지게 됩니다. 바운딩 박스를 사용해 이미지의 문자 영역들을 파이썬 PIL(pillow) 또는 opencv 라이브러리를 사용해 잘라(crop)서 cropped_image_path_list에 담아 리턴하였습니다.\n",
    "\n",
    "(주의) 위 코드에서 lang='kor' 로 바꾸면 에러가 발생합니다. 테서랙트의 언어팩을 설치해야 정상동작하게 됩니다.   \n",
    "$sudo apt install tesseract-ocr-kor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 테서랙트로 잘린 이미지에서 단어 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_images(cropped_image_path_list):\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 7'\n",
    "    \n",
    "    for image_path in cropped_image_path_list:\n",
    "        image = Image.open(image_path)\n",
    "        recognized_data = pytesseract.image_to_string(\n",
    "            image, lang='eng',    # 한국어라면 lang='kor'\n",
    "            config=custom_oem_psm_config,\n",
    "            output_type=Output.DICT\n",
    "        )\n",
    "        print(recognized_data['text'])\n",
    "    print(\"Done\")\n",
    "\n",
    "# 위에서 준비한 문자 영역 파일들을 인식하여 얻어진 텍스트를 출력합니다.\n",
    "recognize_images(cropped_image_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras-ocr 써보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 구글 API를 쓰듯이 OCR을 데모로 써볼 수 있지만 직접 OCR 모델을 만들어 돌려보고 싶을 때는 어떻게 해야 할까요? 저희는 텐서플로우 기반의 모델을 주로 다뤄왔으니 텐서플로우를 기반으로 모델을 구현해서 OCR을 직접 돌려보도록 하겠습니다.\n",
    "\n",
    "keras-ocr은 텐서플로우의 케라스 API를 기반으로 이미지 속 문자를 읽는 End-to-End OCR을 할 수 있게 해줍니다. 공식 문서에도 나와 있듯, 검출 모델로는 네이버 데뷰 2018 영상에서 소개한 CRAFT(Character Region Awareness for Text Detection)를 사용하고, 인식 모델로는 앞에서 설명한 CRNN을 사용합니다.   \n",
    "$ pip install keras-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 라이브러리인 keras_ocr 과 인식결과의 시각화를 위한 matplotlib.pyplot 를 불러옵니다. keras_ocr.pipeline.Pipeline() 는 인식을 위한 파이프라인을 생성하는데 이때 초기화 과정에서 미리 학습된 모델의 가중치(weight)를 불러오게 됩니다. 검출기와 인식기를 위한 가중치 하나씩을 불러오겠지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확인을 위해서 터미널을 열어 다음과 같이 확인해 보시기 바랍니다. true가 출력되어야 환경설정에 반영되어 있는 것입니다. 만약 이 환경설정이 반영되어 있지 않으면 이후 코드 구동 과정에서 OOM(Out Of Memory) 에러가 날 수 있습니다.\n",
    "\n",
    "$ echo $TF_FORCE_GPU_ALLOW_GROWTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras_ocr\n",
    "\n",
    "# keras-ocr이 detector과 recognizer를 위한 모델을 자동으로 다운로드받게 됩니다. \n",
    "pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만들어둔 파이프라인의 recognize() 에 이미지를 몇 가지 넣어줍니다. 예시의 편의 상 url을 읽어 요청하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트에 사용할 이미지 url을 모아 봅니다. 추가로 더 모아볼 수도 있습니다. \n",
    "image_urls = [\n",
    "        'https://upload.wikimedia.org/wikipedia/commons/b/bd/Army_Reserves_Recruitment_Banner_MOD_45156284.jpg',\n",
    "        'https://upload.wikimedia.org/wikipedia/commons/e/e8/FseeG2QeLXo.jpg',\n",
    "        'https://upload.wikimedia.org/wikipedia/commons/b/b4/EUBanana-500x112.jpg'\n",
    "    ]\n",
    "\n",
    "images = [ keras_ocr.tools.read(url) for url in image_urls]\n",
    "prediction_groups = [pipeline.recognize([url]) for url in image_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 인식된 결과를 pyplot으로 시각화를 해봅니다.\n",
    "\n",
    "사용이 매우 간단합니다! 내부적으로 recognize() 는 검출기와 인식기를 두고, 검출기로 바운딩 박스(bounding box, 문자가 있는 영역을 표시한 정보)를 검출한 뒤, 인식기가 각 박스로부터 문자를 인식하는 과정을 거치도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "fig, axs = plt.subplots(nrows=len(images), figsize=(15,15))\n",
    "for idx, ax in enumerate(axs):\n",
    "    keras_ocr.tools.drawAnnotations(image=images[idx], \n",
    "                                    predictions=prediction_groups[idx][0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras-ocr은 한글 데이터셋으로 훈련이 되어있지 않은 모델입니다. 한글 텍스트의 detection은 정상적으로 진행되더라도 recognition 결과가 엉뚱하게 나올 수 있음에 주의해 주세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
