{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification project\n",
    "\n",
    "## Content   \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**1.Aims and objectives**   \n",
    "&nbsp;&nbsp;&nbsp;**2.Literature review**   \n",
    "&nbsp;&nbsp;&nbsp;**3.Method**   \n",
    "&nbsp;&nbsp;&nbsp;**4.Result**   \n",
    "&nbsp;&nbsp;&nbsp;**5.Discussion and conclusion**   \n",
    "&nbsp;&nbsp;&nbsp;**6.Reference**\n",
    "\n",
    "\n",
    "\n",
    "## 1. Aims and objectives\n",
    "\n",
    "This project aims to grasp fundamentals of scikit-learn library, dataset analysis, classification, and evaluation of the result of classification. Scikit-learn provides small sets of standard datasets, for example boston house prices for regression and diabetes dataset for classification. In this project, three classification datasets, iris, wine, and breast cancer datasets, had been adopted and analysed, and the models which were trained with them was evaluated accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Literature review\n",
    "### Scikit-learn   \n",
    "   \n",
    "Scikit-learn is a free machine learning library for python programming language. It involves commonly used classification, clustering, and regression algorithms such as Suppor Vector Machine(SVM), decision tree, random forest or K-means method. The library was designed to interact with Numpy and Scipy perform both supervised and unsupervised learning (see [Scikit-learn](https://scikit-learn.org/stable/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and analyse datasets   \n",
    "\n",
    "Three classification datasets(hand written digits, wine, breast cancer) had been acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report will focus on wine classification. For full codes of the three practices, ssee [wine](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Wine_Classification.ipynb), [handwritten digits](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Handwritten_Letter_Classifier.ipynb), and [breast cancer](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Breast_Cancer_Classifier.ipynb).   \n",
    "\n",
    "Loading the wine dataset, the samples and labels (targets) were assigned into 'wine_data' and 'wine_label' respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  178 \n",
      "Feature names:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline'] \n",
      "Label names:  ['class_0' 'class_1' 'class_2']\n",
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wines = load_wine()\n",
    "\n",
    "wines_data = wines.data # assign the data\n",
    "wines_label = wines.target # assign the label\n",
    "\n",
    "\n",
    "print('Number of samples: ', len(wines.data), '\\nFeature names: ', wines.feature_names, '\\nLabel names: ', wines.target_names)\n",
    "print(wines.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed above, the wine dataset has 178 samples, 3 classes, and 13 features. Knowing how the dataset constitues is salient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset partition   \n",
    "   \n",
    "The dataset needs to be divided into both training and test dataset. The splitting can be achieved with 'train_test_split' function and only 20 percent of the dataset was converted to the test set. Moreover, when it comes to partitioning samples, it must be randomly executed since unshuffled subsets could result in unwelcomed bias into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wines_data, \n",
    "                                                   wines_label,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "   \n",
    "Evaluating the model is paramount importance in classification. Above all, an optimized classifier is obtained by correct selection of appropriate metric for the optimal solution [1]    \n",
    "##### i) Accuracy   \n",
    "Accuracy is one of the most commonly used metrices among the researchers.  \n",
    "\n",
    "$${Accuracy = \\frac{Number\\,of\\,correct\\, predictions}{Total\\, number\\, of\\, predictions\\, made}}$$  \n",
    "\n",
    "However, considering accuracy as the main evaluation metric will come with a number of limitations. Especially, [2] and [3] have well demonstrated how accuracy is not good at tackling imbalanced dataset.   \n",
    "   \n",
    "##### ii) Confusion matrix   \n",
    "Confusion matrix is a matrix that suitably summarises the performance of the algorithm. In particular, it is competent at representing where the model have been confused in predicting.   \n",
    "![Confusion matrix](./r4.jpg)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *Figure 1: Confusion Matrix*   \n",
    "   \n",
    "The four outputs in figure 1 represent significant milestones when evaluating the performance.\n",
    "- True Positive(TP): The number of correct predictions that the model predicted true as true.   \n",
    "- False Negative(FN): The number of incorrect predictions that the model predicted positive as negative.   \n",
    "- False Positive(FP): The number of incorrect predictions that the model predicted negative as positive.   \n",
    "- True Negative(TN): The number of correct predictions that hte model predicted negative as negative.   \n",
    "They are mathematically calculated so as to express another important indications.   \n",
    "- Recall(Sensitivity): Recall is a ratio of 'TP' to 'actual' positive samples.   \n",
    "   \n",
    "$${Recall = \\frac{TP}{TP+FN}}$$   \n",
    "\n",
    "- Specificity: Specificity is a ratio of 'TN' to 'actual' negative samples.   \n",
    "\n",
    "$${Specificity = \\frac{TN}{FP+TN}}$$   \n",
    "\n",
    "- Precision: Precision is a ratio of 'TP' to 'labelled' positive samples.   \n",
    "\n",
    "$${Precision = \\frac{TP}{TP+FP}}$$   \n",
    "\n",
    "- F1 score: F1 score is a ratio showing the balance between Precision and Recall.   \n",
    "\n",
    "$${F1\\, score = 2*\\frac{Precision*Recall}{Precision+Recall}}$$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method   \n",
    "### Training the model   \n",
    "   \n",
    "Five prominent classification algorithms had been trained: Decision tree, Random forest, Support Vector Machine, Stochastic Gradient Descent, and Logistic Regression. Each algorithm asks for different syntax to call the classifier model.\n",
    "\n",
    "##### i) Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii) Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iv) Stochastic Gradient Descent Classifier(SGD Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result   \n",
    "\n",
    "For the results, see [wine](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Wine_Classification.ipynb), [handwritten digits](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Handwritten_Letter_Classifier.ipynb), and [breast cancer](https://github.com/hweejuni/The-very-first-repository/blob/master/Classifier_Project/Breast_Cancer_Classifier.ipynb).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and conclusion   \n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference   \n",
    "\n",
    "[1] H. M and S. M.N, \"A Review on Evaluation Metrics for Data Classification Evaluations\", International Journal of Data Mining & Knowledge Management Process, vol. 5, no. 2, pp. 01-11, 2015. Available: 10.5121/ijdkp.2015.5201 [Accessed 30 July 2020].   \n",
    "\n",
    "[2] R. Ranawana, and V. Palade, “Optimized precision-A new measure for classifier performance\n",
    "evaluation”, in Proc. of the IEEE World Congress on Evolutionary Computation (CEC 2006), 2006,\n",
    "pp. 2254-2261.    \n",
    "\n",
    "[3] S. W. Wilson, “Mining oblique data with XCS”, in P. L. Lanzi, W. Stolzmann and S. W. Wilson\n",
    "(Eds.) Advances in Learning Classifier Systems: Third Int. Workshop (IWLCS 2000), Berlin,\n",
    "Heidelberg: Springer-Verlag, 2001, pp. 283-290. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
